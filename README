# Predicting Customer Purchase Value from Multi-Session Web Behaviour

A complete, CPU-friendly ML pipeline with rich EDA, feature engineering & a two-stage ensemble achieving **RÂ² â‰ˆ 0.86** on hold-out validation.

---

## ðŸ“‘ Table of Contents
1. [Overview](#overview)
2. [Dataset](#dataset)
3. [Project Highlights](#project-highlights)
4. [Exploratory Data Analysis (EDA)](#exploratory-data-analysis-eda)
5. [Feature Engineering](#feature-engineering)
6. [Modelling Strategy](#modelling-strategy)
7. [Results](#results)
8. [Repository Structure](#repository-structure)
9. [How to Run](#how-to-run)
10. [Future Work](#future-work)

---

## Overview

Online retailers collect millions of click-stream events, yet only a small fraction translate into revenue.
This project builds a production-ready, CPU-only pipeline that predicts each userâ€™s `purchaseValue` from their multi-session behaviour.

### Key Objectives ðŸ’¡

| # | Goal | Met? |
|---|------|------|
| 1 | Perform thorough EDA to reveal data quirks | âœ… |
| 2 | Engineer robust numeric & categorical features | âœ… |
| 3 | Train â‰¥ 3 diverse models and analyse them | âœ… (Ridge, RandomForest, SGD, MLP, XGBoost) |
| 4 | Tune a final ensemble without GPUs | âœ… (Two-stage XGBoost + calibrated classifier) |
| 5 | Achieve RÂ² â‰¥ 0.75 on a held-out fold | âœ… (~ 0.864 Â± 0.01) |

---

## Dataset

### ðŸ“Š **Dataset Statistics**
- **Train:** 116,023 rows Ã— 52 columns
- **Test:** 29,006 rows Ã— 51 columns (no target column)
- **Target:** `purchaseValue` (continuous, heavily right-skewed)

### ðŸ” **Data Quality Insights**

**Missing Data Analysis:**
- **Highest missing rates (Train):**
  - `trafficSource.adContent`: 97.45%
  - `trafficSource.adwordsClickInfo.isVideoAd`: 96.31%
  - `trafficSource.adwordsClickInfo.page`: 96.31%
  - `trafficSource.adwordsClickInfo.adNetworkType`: 96.31%
  - `trafficSource.adwordsClickInfo.slot`: 96.31%

- **Highest missing rates (Test):**
  - `trafficSource.adContent`: 97.38%
  - `trafficSource.adwordsClickInfo.adNetworkType`: 96.18%
  - `trafficSource.adwordsClickInfo.isVideoAd`: 96.18%
  - `trafficSource.adwordsClickInfo.page`: 96.18%
  - `trafficSource.adwordsClickInfo.slot`: 96.18%

**Top Feature Correlations with Target:**
1. `totalHits` - Strongest predictor
2. `pageViews` - Second strongest
3. `sessionNumber` - Third strongest
4. `trafficSource.adwordsClickInfo.page` - Fourth strongest

### ðŸ·ï¸ **Feature Categories**
- **Target** â€“ `purchaseValue` (continuous, heavily right-skewed)
- **User behaviour** â€“ `pageViews`, `totalHits`, `sessionNumber`, etc.
- **Traffic source** â€“ ad network hints, referral paths, keywords
- **Device / geo** â€“ browser, OS, country, mobile flags
- **Time** â€“ daily QSIDs mapped to month/week-day

> **Note:** Missingness is extreme in ad-traffic columns (â‰ˆ 96-97%), indicating these features are primarily available for paid advertising traffic.

---

## Project Highlights

- **CPU-friendly:** pure NumPy / Sci-kits / XGBoost-CPU
- **Two-Stage Architecture:**
    1. Classifier predicts â€œwill buy?â€ (Calibrated XGB)
    2. Regressor predicts log-purchase for buyers; final prediction = p(buy) * exp(log_pred)
- **Proven generalisation:** 20% hold-out split & early-stopping
- **Model zoo for viva/Q&A:** Ridge, RF, SGD, MLP, plus optional Naive Bayes / KNN / SVM for the classifier
- **Clean, reproducible notebook** with comments, pipelines & plots

---

## Exploratory Data Analysis (EDA)

| Insight | Evidence |
|---------|----------|
| Target extremely long-tailed | log-histogram & 99th percentile cap (â‰ˆ 4.8 Ã— 10â¸) |
| Top numeric correlates | totalHits, pageViews, sessionNumber, trafficSource.adwordsClickInfo.page |
| High missingness (â‰¥ 96%) in ad-traffic features | bar-plot of NaN % |
| Week-end effect | mean purchase is ~25% lower on Sat/Sun |
| Device.isMobile weak predictor alone, but strong after interaction with ad flags | pair-plot |

> Heat-map (masking NaNs) revealed no catastrophic multi-collinearity after dropping constants.

---

## Feature Engineering

| Step | Why |
|------|-----|
| Duplicate & constant-column removal | trim noise + RAM |
| Binary conversion for bool/TF columns | model-agnostic numerics |
| User-level aggregates (u_pg_mean, u_sess_count, â€¦) | capture cross-session intent |
| Date features (month, weekday, is_weekend) | seasonality |
| Log1p numeric transform | stabilise skew |
| Target encoding for high-cardinality cats | avoid OHE explosion |
| Poly interactions on top-5 nums | cheap non-linearities |

---

## Modelling Strategy

```
                 +--------------------+
 Raw â†’ Clean â†’ FE â†’  |  XGB Classifier   |  â†’  p(buy)
                 +--------------------+
                            â”‚
                            â–¼
                 +--------------------+
                 |  XGB Regressor     |  â†’  log(ð‘¦Ì‚)  (buyers only)
                 +--------------------+

 Final Å· = p(buy) Ã— exp(log(ð‘¦Ì‚))
```

- **Early-stopping rounds:** max(10, 5% of buyer-validation rows)

**Baseline RÂ² on buyers:**
- Ridge ~ 0.916
- RandomForest ~ 0.934
- SGD ~ 0.911
- MLP ~ 0.906

> Hyper-params hard-coded after an offline grid-search script (see `HyperParams.py`).

---

## Results

### ðŸŽ¯ **Final Performance Metrics**

| Metric | Validation (20%) | Details |
|--------|------------------|---------|
| **Two-stage ensemble RÂ²** | **0.8637** | Final tuned model |
| Classifier accuracy | 0.9933 | SVM (RBF) - best performer |
| PCA retained comps | 7 | 93% variance explained |
| Target 99th percentile cap | 483,870,000 | Log1p transformation threshold |

### ðŸ“Š **Model Comparison Results**

**Classifier Performance (Binary - Will Buy?):**
- **SVM (RBF)**: 0.9933 accuracy
- **K-Nearest Neighbors**: 0.9927 accuracy  
- **Gaussian Naive Bayes**: 0.9798 accuracy

**Regression Performance (Buyers Only - Purchase Value):**
- **Random Forest**: 0.9339 RÂ²
- **Ridge Regression**: 0.9163 RÂ²
- **SGD Regressor**: 0.9111 RÂ²
- **MLP Neural Network**: 0.9063 RÂ²

### ðŸ”§ **Hyperparameter Tuning Results**

**Coarse Tuning:**
- Best subsample: 0.6
- Best colsample_bytree: 0.8  
- Best min_child_weight: 10
- RÂ²: 0.8580

**Refined Tuning:**
- Learning rate: 0.1
- Max depth: 8
- Gamma: 0
- Lambda: 10
- **Final RÂ²: 0.8637**

> The score comfortably beats the (75%) viva threshold and demonstrates excellent generalization.

> ðŸ“‹ **For detailed results, hyperparameter tuning analysis, and complete output logs, see [RESULTS.md](RESULTS.md)**

---

## Repository Structure

```bash
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train_data.csv
â”‚   â””â”€â”€ test_data.csv
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ submission.csv          # Generated predictions
â”‚   â”œâ”€â”€ sample_submission.csv   # Sample output format
â”‚   â””â”€â”€ .gitkeep               # Preserves directory structure
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ Predictor.py           # Main prediction pipeline
â”‚   â””â”€â”€ HyperParams.py         # Hyperparameter tuning script
â”œâ”€â”€ README.md                  # Project overview & setup
â”œâ”€â”€ RESULTS.md                 # Detailed results & analysis
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ .gitignore                # Git ignore rules
```

---

## How to Run

```bash
# 1. Clone repo & install deps (CPU only)
pip install -r requirements.txt

# 2. Place raw CSVs into the repo folder (train_data.csv, test_data.csv)

# 3. Run the predictor script:
python src/Predictor.py

# 4. Check outputs:
# - Generated submission file: output/submission.csv
# - Sample format: output/sample_submission.csv
```

All heavy lifting happens on a single CPU core; memory footprint < 3 GB.

---

## Future Work

- Cross-session RNN / Transformer to capture sequential events
- SHAP-based feature pruning for interpretability
- Live A/B pipeline to update target encodings weekly

---

## âœ¨ Take-away

A compact, well-documented workflow that:
- âœ“ Performs full EDA
- âœ“ Implements rich feature engineering
- âœ“ Compares multiple algorithms
- âœ“ Tunes & deploys a high-performing, CPU-only ensemble
