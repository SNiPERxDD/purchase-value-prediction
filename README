# Predicting Customer Purchase Value from Multi-Session Web Behaviour

A complete, CPU-friendly ML pipeline with rich EDA, feature engineering & a two-stage ensemble achieving **R² ≈ 0.86** on hold-out validation.

---

## 📑 Table of Contents
1. [Overview](#overview)
2. [Dataset](#dataset)
3. [Project Highlights](#project-highlights)
4. [Exploratory Data Analysis (EDA)](#exploratory-data-analysis-eda)
5. [Feature Engineering](#feature-engineering)
6. [Modelling Strategy](#modelling-strategy)
7. [Results](#results)
8. [Repository Structure](#repository-structure)
9. [How to Run](#how-to-run)
10. [Future Work](#future-work)

---

## Overview

Online retailers collect millions of click-stream events, yet only a small fraction translate into revenue.
This project builds a production-ready, CPU-only pipeline that predicts each user’s `purchaseValue` from their multi-session behaviour.

### Key Objectives 💡

| # | Goal | Met? |
|---|------|------|
| 1 | Perform thorough EDA to reveal data quirks | ✅ |
| 2 | Engineer robust numeric & categorical features | ✅ |
| 3 | Train ≥ 3 diverse models and analyse them | ✅ (Ridge, RandomForest, SGD, MLP, XGBoost) |
| 4 | Tune a final ensemble without GPUs | ✅ (Two-stage XGBoost + calibrated classifier) |
| 5 | Achieve R² ≥ 0.75 on a held-out fold | ✅ (~ 0.864 ± 0.01) |

---

## Dataset

### 📊 **Dataset Statistics**
- **Train:** 116,023 rows × 52 columns
- **Test:** 29,006 rows × 51 columns (no target column)
- **Target:** `purchaseValue` (continuous, heavily right-skewed)

### 🔍 **Data Quality Insights**

**Missing Data Analysis:**
- **Highest missing rates (Train):**
  - `trafficSource.adContent`: 97.45%
  - `trafficSource.adwordsClickInfo.isVideoAd`: 96.31%
  - `trafficSource.adwordsClickInfo.page`: 96.31%
  - `trafficSource.adwordsClickInfo.adNetworkType`: 96.31%
  - `trafficSource.adwordsClickInfo.slot`: 96.31%

- **Highest missing rates (Test):**
  - `trafficSource.adContent`: 97.38%
  - `trafficSource.adwordsClickInfo.adNetworkType`: 96.18%
  - `trafficSource.adwordsClickInfo.isVideoAd`: 96.18%
  - `trafficSource.adwordsClickInfo.page`: 96.18%
  - `trafficSource.adwordsClickInfo.slot`: 96.18%

**Top Feature Correlations with Target:**
1. `totalHits` - Strongest predictor
2. `pageViews` - Second strongest
3. `sessionNumber` - Third strongest
4. `trafficSource.adwordsClickInfo.page` - Fourth strongest

### 🏷️ **Feature Categories**
- **Target** – `purchaseValue` (continuous, heavily right-skewed)
- **User behaviour** – `pageViews`, `totalHits`, `sessionNumber`, etc.
- **Traffic source** – ad network hints, referral paths, keywords
- **Device / geo** – browser, OS, country, mobile flags
- **Time** – daily QSIDs mapped to month/week-day

> **Note:** Missingness is extreme in ad-traffic columns (≈ 96-97%), indicating these features are primarily available for paid advertising traffic.

---

## Project Highlights

- **CPU-friendly:** pure NumPy / Sci-kits / XGBoost-CPU
- **Two-Stage Architecture:**
    1. Classifier predicts “will buy?” (Calibrated XGB)
    2. Regressor predicts log-purchase for buyers; final prediction = p(buy) * exp(log_pred)
- **Proven generalisation:** 20% hold-out split & early-stopping
- **Model zoo for viva/Q&A:** Ridge, RF, SGD, MLP, plus optional Naive Bayes / KNN / SVM for the classifier
- **Clean, reproducible notebook** with comments, pipelines & plots

---

## Exploratory Data Analysis (EDA)

| Insight | Evidence |
|---------|----------|
| Target extremely long-tailed | log-histogram & 99th percentile cap (≈ 4.8 × 10⁸) |
| Top numeric correlates | totalHits, pageViews, sessionNumber, trafficSource.adwordsClickInfo.page |
| High missingness (≥ 96%) in ad-traffic features | bar-plot of NaN % |
| Week-end effect | mean purchase is ~25% lower on Sat/Sun |
| Device.isMobile weak predictor alone, but strong after interaction with ad flags | pair-plot |

> Heat-map (masking NaNs) revealed no catastrophic multi-collinearity after dropping constants.

---

## Feature Engineering

| Step | Why |
|------|-----|
| Duplicate & constant-column removal | trim noise + RAM |
| Binary conversion for bool/TF columns | model-agnostic numerics |
| User-level aggregates (u_pg_mean, u_sess_count, …) | capture cross-session intent |
| Date features (month, weekday, is_weekend) | seasonality |
| Log1p numeric transform | stabilise skew |
| Target encoding for high-cardinality cats | avoid OHE explosion |
| Poly interactions on top-5 nums | cheap non-linearities |

---

## Modelling Strategy

```
                 +--------------------+
 Raw → Clean → FE →  |  XGB Classifier   |  →  p(buy)
                 +--------------------+
                            │
                            ▼
                 +--------------------+
                 |  XGB Regressor     |  →  log(𝑦̂)  (buyers only)
                 +--------------------+

 Final ŷ = p(buy) × exp(log(𝑦̂))
```

- **Early-stopping rounds:** max(10, 5% of buyer-validation rows)

**Baseline R² on buyers:**
- Ridge ~ 0.916
- RandomForest ~ 0.934
- SGD ~ 0.911
- MLP ~ 0.906

> Hyper-params hard-coded after an offline grid-search script (see `HyperParams.py`).

---

## Results

### 🎯 **Final Performance Metrics**

| Metric | Validation (20%) | Details |
|--------|------------------|---------|
| **Two-stage ensemble R²** | **0.8637** | Final tuned model |
| Classifier accuracy | 0.9933 | SVM (RBF) - best performer |
| PCA retained comps | 7 | 93% variance explained |
| Target 99th percentile cap | 483,870,000 | Log1p transformation threshold |

### 📊 **Model Comparison Results**

**Classifier Performance (Binary - Will Buy?):**
- **SVM (RBF)**: 0.9933 accuracy
- **K-Nearest Neighbors**: 0.9927 accuracy  
- **Gaussian Naive Bayes**: 0.9798 accuracy

**Regression Performance (Buyers Only - Purchase Value):**
- **Random Forest**: 0.9339 R²
- **Ridge Regression**: 0.9163 R²
- **SGD Regressor**: 0.9111 R²
- **MLP Neural Network**: 0.9063 R²

### 🔧 **Hyperparameter Tuning Results**

**Coarse Tuning:**
- Best subsample: 0.6
- Best colsample_bytree: 0.8  
- Best min_child_weight: 10
- R²: 0.8580

**Refined Tuning:**
- Learning rate: 0.1
- Max depth: 8
- Gamma: 0
- Lambda: 10
- **Final R²: 0.8637**

> The score comfortably beats the (75%) viva threshold and demonstrates excellent generalization.

> 📋 **For detailed results, hyperparameter tuning analysis, and complete output logs, see [RESULTS.md](RESULTS.md)**

---

## Repository Structure

```bash
.
├── data/
│   ├── train_data.csv
│   └── test_data.csv
├── output/
│   ├── submission.csv          # Generated predictions
│   ├── sample_submission.csv   # Sample output format
│   └── .gitkeep               # Preserves directory structure
├── src/
│   ├── Predictor.py           # Main prediction pipeline
│   └── HyperParams.py         # Hyperparameter tuning script
├── README.md                  # Project overview & setup
├── RESULTS.md                 # Detailed results & analysis
├── requirements.txt           # Python dependencies
└── .gitignore                # Git ignore rules
```

---

## How to Run

```bash
# 1. Clone repo & install deps (CPU only)
pip install -r requirements.txt

# 2. Place raw CSVs into the repo folder (train_data.csv, test_data.csv)

# 3. Run the predictor script:
python src/Predictor.py

# 4. Check outputs:
# - Generated submission file: output/submission.csv
# - Sample format: output/sample_submission.csv
```

All heavy lifting happens on a single CPU core; memory footprint < 3 GB.

---

## Future Work

- Cross-session RNN / Transformer to capture sequential events
- SHAP-based feature pruning for interpretability
- Live A/B pipeline to update target encodings weekly

---

## ✨ Take-away

A compact, well-documented workflow that:
- ✓ Performs full EDA
- ✓ Implements rich feature engineering
- ✓ Compares multiple algorithms
- ✓ Tunes & deploys a high-performing, CPU-only ensemble
